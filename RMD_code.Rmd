---
title:  "Predicting Titanic Survivors using Logistic Regression"
author: "John Soler"
date:   "31 March 2019"
output: html_document
---

This project is carried out and presented here to display: my statistical knowledge particularly applied to Logistic Regression; my level of programming in R; and ability to explain and document advanced numerical techniques. I would consider this report as moderately technical and it is intended for an audience having a basic understanding of Regression as well as interest in some of the underlying concepts to this algorithm.

The problem tackled here is to predict, as accurately as possible, the fate of a set of passengers on board the Titanic; whether they would survive or not (i.e. a binary classification problem). I obtained labelled data from Kaggle in the challenge "Titanic: Machine Learning from Disaster" (https://www.kaggle.com/c/titanic), and then applied my algorithm to a test set, also provided and evaluated by Kaggle.

This report explains the approach taken - including feature engineering, catering for multicollinearity, regularisation, etc. - as well as the final model. Kaggle's evaluation determined an accuracy of just over 78% for my final Logistic Regression model. Furthermore, in some areas of the report I took the opportunity to deviate slightly to explore some concepts related to Regression in general. The same exact reasoning discussed for this problem could be applied to a Churn Prediction model, Insurance Pricing model, or any other propensity model valuable for business. 


## Contents

1. Initialisation
2. Data Cleaning
3. Data Exploration, Feature Engineering & Scaling
4. Multi-Collinearity & Feature Selection
5. Model building
6. Model Evaluation


##1. Initialisation

The first section of the R code includes setting up packages, extracting data and making sure that the report is reproducible by setting a seed. The data is in CSV format and can be extracted drictly from Kaggle, as well as from my GitHub account.

```{r setting_up, echo=FALSE, message = F, chache = TRUE}

set.seed(200591)

packagesNeeded <- c("RCurl", "dplyr", "stringr", "ggplot2", "reshape"
                    , "glmnet", "caret", "car", "pROC", "knitr")

for (i in packagesNeeded){
  if(!(i %in% installed.packages())){
    install.packages(i)
  }
  library(i, character.only = T)
}

rawTrain <- getURL("https://raw.githubusercontent.com/solerj/Kaggle_Titanic/master/train.csv")
trainO <- read.csv(text = rawTrain)
rawTest <- getURL("https://raw.githubusercontent.com/solerj/Kaggle_Titanic/master/test.csv")
testO <- read.csv(text = rawTest)

testO$Survived <- NA
dataAll <- rbind(trainO, testO)
```


##2. Data Cleaning

First, here is the raw data with some information on each variable as described in Kaggle:

```{r view_data, echo=FALSE, cache=TRUE, message = F}

dataInfo <- cbind(colnames(dataAll)
                  , c("Passenger Number"
                      , "Survived or Not - the Target variable"
                      , "Class of Travel"
                      , "Name of Passenger"
                      , "Gender"
                      , "Age of Passengers"
                      , "Number of Sibling/Spouse aboard"
                      , "Number of Parent/Child aboard"
                      , "Ticket code"
                      , "Fare"
                      , "Cabin code"
                      , "The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"))

colnames(dataInfo) <- c("Variable", "Description")
kable(dataInfo, caption = "Data Description")

```


A high level data cleaning is carried out. This includes preparing data types and some simple data manipulation as follows:

* Extracted the title of each passenger from the Name variable, and called the new feature `NameTitle`
* Filled in a few missing values for the Fare and Embarked variables
* Transformed the variables: `Pclass`, `Sex`, `Embarked` & `NameTitle` into factors. I made sure that for each variable, the most common level is labelled as the primary level. This is important in anticipation of fitting the regression model.


```{r clean, echo=FALSE, cache=TRUE, message = F}

getTitle <- function(name) {
  firstTitle <- str_sub(name,
                        start = (str_locate(name, ',')[1]+2),
                        end = (str_locate(name, '[.]')[1]-1))
  firstTitle
}
dataAll$Name      <- as.character(dataAll$Name)
dataAll$NameTitle <- sapply(dataAll$Name, getTitle)


fareMissing <- dataAll[is.na(dataAll$Fare),]
fareQuickFill <- median(dataAll[dataAll$NameTitle == "Mr" &
                                  dataAll$Pclass == 3 &
                                  dataAll$Embarked == "S"
                                , c("Fare")], na.rm = T)
dataAll$Fare[is.na(dataAll$Fare)] <- fareQuickFill


embarkedMissing <- dataAll[dataAll$Embarked == "",]
tbEmbark <- table(dataAll$Embarked[dataAll$Pclass == 1 &
                                     dataAll$SibSp == 0 &
                                     dataAll$Parch == 0])
modeEmbarked <- names(which.max(tbEmbark))
dataAll$Embarked[dataAll$Embarked == ""] <- modeEmbarked


myAsFactor <- function(tableName, variable){
  if(is.factor(tableName[,c(variable)])){
    tableName[,c(variable)] <- as.character(tableName[,c(variable)])
  }
  tb <- table(tableName[,c(variable)])
  factor(tableName[,c(variable)]
         , levels = names(tb[order(tb, decreasing = TRUE)]))
}

dataAll$SurvivedF <- myAsFactor(dataAll, "Survived")
variablesToRefactor <- c("Pclass", "Sex", "Embarked", "NameTitle")
for (variable in variablesToRefactor){
  dataAll[,c(variable)] <- myAsFactor(dataAll, variable)
}

checkDependence <- function(variable1, variable2){
  contingency <- table(dataAll[,c(variable1, variable2)])
  chisq <- chisq.test(x = contingency, simulate.p.value = T)
  chisq$p.value
}

```


##3. Data Exploration

Here I investigate each variable in more detail, particularly in view of the Target variable. For each variable, I look for any relationship with the Target by using the Chi-Squared Test of Independence if the variable is categorical (e.g. Embarked) or using the Student's t-Test if the variable is numerical (e.g. Fare). When categorical variables have a lot of levels, I consider grouping levels. I also apply scaling of numerical variables as necessary.

```{r class, echo=FALSE, cache=TRUE, message = F}

table(dataAll$Pclass)
barClassSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x=Pclass
                             , y=Survived))
barClassSurv <- barClassSurv + stat_summary(fun.y="mean", geom="bar")
barClassSurv <- barClassSurv + ylim(0, 1)
barClassSurv
checkDependence("SurvivedF", "Pclass")

```

```{r cabinLetter, echo=FALSE, cache=TRUE, message = F}

dataAll$cabinLetter <- str_sub(dataAll$Cabin, 1, 1)
table(dataAll$cabinLetter)
dataAll$cabinLetter[dataAll$cabinLetter == ""] <- "N"
barCabinSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x=reorder(cabinLetter,-Survived,mean)
                             , y=Survived))
barCabinSurv <- barCabinSurv + stat_summary(fun.y="mean", geom="bar")
barCabinSurv <- barCabinSurv + ylim(0, 1)
barCabinSurv
dataAll$cabinLetter <- myAsFactor(dataAll, "cabinLetter")
checkDependence("SurvivedF", "cabinLetter")

```

```{r noCabins, echo=FALSE, cache=TRUE, message = F}

dataAll$noCabins <- str_count(as.character(dataAll$Cabin), " ")
table(dataAll$noCabins)
barNoCabinSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x = noCabins
                             , y=Survived))
barNoCabinSurv <- barNoCabinSurv + stat_summary(fun.y="mean", geom="bar")
barNoCabinSurv <- barNoCabinSurv + ylim(0, 1)
barNoCabinSurv
checkDependence("SurvivedF", "noCabins")

dataAll$noCabinsGroup <- dataAll$noCabins
dataAll$noCabinsGroup[dataAll$noCabins != 0] <- "1+"
dataAll$noCabinsGroup <- myAsFactor(dataAll, "noCabinsGroup")
checkDependence("SurvivedF", "noCabinsGroup")

```

```{r NameTitles, echo=FALSE, cache=TRUE, message = F}

table(dataAll$NameTitle)

# clearly some relationship between NameTitle and probab of Survival
# but there are a bit too many categories and risk of overfitting
barTitleSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x=reorder(NameTitle,-Survived,mean)
                             , y=Survived))
barTitleSurv <- barTitleSurv + stat_summary(fun.y="mean", geom="bar")
barTitleSurv <- barTitleSurv + ylim(0, 1)
barTitleSurv

# created title groups for more important people 
dataAll$titleGroup <- as.character(dataAll$NameTitle)
dataAll$titleGroup[dataAll$NameTitle %in% c("Col", "Don", "Dr", "Jonkheer", "Major", "Rev", "Sir")
                   & dataAll$Sex == "male"] <- "Dr"
dataAll$titleGroup[dataAll$NameTitle %in% c("Dona", "Dr", "Lady", "Mlle", "Mme", "the Countess")
                   & as.character(dataAll$Sex) == "female"] <- "Mme"
dataAll$titleGroup[dataAll$NameTitle %in% c("Ms")
                   & as.character(dataAll$Sex) == "female"] <- "Mrs"
dataAll$titleGroup <- myAsFactor(dataAll, "titleGroup")
table(dataAll$titleGroup)


barTitleGrpSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                          , aes(x = reorder(titleGroup, -Survived, mean)
                                , y = Survived))
barTitleGrpSurv <- barTitleGrpSurv + stat_summary(fun.y="mean", geom="bar")
barTitleGrpSurv <- barTitleGrpSurv + ylim(0, 1)
barTitleGrpSurv
checkDependence("SurvivedF", "titleGroup")

```

```{r Age, echo=FALSE, cache=TRUE, message = F}

# 5. is Age related to Survival? ----------------------------------------------
# first note that there are many NAs for age
# if age turns out to be significant, we will want to do something about that
summary(dataAll$Age)


lineAgeSurv <- ggplot(dataAll[!is.na(dataAll$Age) & !is.na(dataAll$Survived),]
                      , aes(x = Age
                            , y = Survived
                            , colour = Sex))
lineAgeSurv <- lineAgeSurv + stat_summary_bin(bins = 50, fun.y="mean", geom="line", size = 0.2)
lineAgeSurv <- lineAgeSurv + stat_summary_bin(bins = 30, fun.y="mean", geom="line", size = 0.5)
lineAgeSurv <- lineAgeSurv + stat_summary_bin(bins = 20, fun.y="mean", geom="line", size = 0.75)
lineAgeSurv <- lineAgeSurv + stat_summary_bin(bins = 10, fun.y="mean", geom="line", size = 1)
lineAgeSurv

# so yes, there is so dependency, especially on younger males
# but these are probably labelled "Master"
# so split age-survival relationship per titleGroup
# then relationship is much less evident

lineAgeTitleSurv <- ggplot(dataAll[!is.na(dataAll$Age)
                              & !is.na(dataAll$Survived)
                              & dataAll$titleGroup != "Capt",]
                      , aes(x = Age
                            , y = Survived
                            , colour = Sex))
lineAgeTitleSurv <- lineAgeTitleSurv + facet_grid(rows = vars(titleGroup))
lineAgeTitleSurv <- lineAgeTitleSurv + stat_summary_bin(bins = 50, fun.y="mean", geom="line", size = 0.2)
lineAgeTitleSurv <- lineAgeTitleSurv + stat_summary_bin(bins = 30, fun.y="mean", geom="line", size = 0.5)
lineAgeTitleSurv <- lineAgeTitleSurv + stat_summary_bin(bins = 20, fun.y="mean", geom="line", size = 0.75)
lineAgeTitleSurv <- lineAgeTitleSurv + stat_summary_bin(bins = 10, fun.y="mean", geom="line", size = 1)
#lineAgeTitleSurv <- lineAgeTitleSurv + geom_smooth(method = "lm")
lineAgeTitleSurv


dataAll$ageBucket <- NA
dataAll$ageBucket[dataAll$Age < 3] <- "0-3"
dataAll$ageBucket[dataAll$Age >= 3 & dataAll$Age < 6] <- "3-6"
dataAll$ageBucket[dataAll$Age >= 6 & dataAll$Age < 9] <- "6-9"
dataAll$ageBucket[dataAll$Age >= 9 & dataAll$Age < 12] <- "9-12"
dataAll$ageBucket[dataAll$Age >= 12 & dataAll$Age < 15] <- "12-15"
dataAll$ageBucket[dataAll$Age >= 15] <- "15+"
dataAll$ageBucket <- myAsFactor(dataAll, "ageBucket")

summary(dataAll$ageBucket)
barAgeSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                     , aes(x = ageBucket
                           , y = Survived))
#barEmbSurv <- barEmbSurv + facet_grid(cols = vars(cabinClass))
barAgeSurv <- barAgeSurv + stat_summary(fun.y="mean", geom="bar")
barAgeSurv <- barAgeSurv + ylim(0, 1)
barAgeSurv
checkDependence("SurvivedF", "ageBucket")

```

```{r Embarking, echo=FALSE, cache=TRUE, message = F}

table(dataAll$Embarked)
barEmbSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                     , aes(x = Embarked
                           , y = Survived))
#barEmbSurv <- barEmbSurv + facet_grid(cols = vars(cabinClass))
barEmbSurv <- barEmbSurv + stat_summary(fun.y="mean", geom="bar")
barEmbSurv <- barEmbSurv + ylim(0, 1)
barEmbSurv
checkDependence("SurvivedF", "Embarked")

```

```{r Fare, echo=FALSE, cache=TRUE, message = F}

summary(dataAll$Fare)
hist(dataAll$Fare)
hist(log(dataAll$Fare+1))
dataAll$logFare <- log(dataAll$Fare + 1)


scatFareSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x = logFare
                             , y = Survived))
scatFareSurv <- scatFareSurv + stat_summary_bin(bins = 50, fun.y="mean", geom="line", size = 0.2)
scatFareSurv <- scatFareSurv + stat_summary_bin(bins = 30, fun.y="mean", geom="line", size = 0.5)
scatFareSurv <- scatFareSurv + stat_summary_bin(bins = 20, fun.y="mean", geom="line", size = 0.75)
scatFareSurv <- scatFareSurv + stat_summary_bin(bins = 10, fun.y="mean", geom="line", size = 1)
scatFareSurv <- scatFareSurv + geom_smooth(method = "lm")
scatFareSurv


medlogFare <- median(dataAll$logFare)
dataAll$logFareS <- dataAll$logFare / (2*medlogFare)
summary(dataAll$logFareS)

```

```{r Parch, echo=FALSE, cache=TRUE, message = F}

table(dataAll$Parch)
table(dataAll$Parch[!is.na(dataAll$Survived)])
barParchSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x = Parch
                             , y = Survived))
barParchSurv <- barParchSurv + stat_summary(fun.y="mean", geom="bar")
barParchSurv <- barParchSurv + ylim(0, 1)
barParchSurv

dataAll$parchGroup <- dataAll$Parch
dataAll$parchGroup[dataAll$Parch %in% c(1,2)] <- "1_2"
dataAll$parchGroup[dataAll$Parch >= 3] <- "3+"
dataAll$parchGroup <- myAsFactor(dataAll, "parchGroup")
table(dataAll$parchGroup)
checkDependence("SurvivedF", "parchGroup")

```

```{r SibSp, echo=FALSE, cache=TRUE, message = F}

table(dataAll$SibSp)
table(dataAll$SibSp[!is.na(dataAll$Survived)])
barSibSpSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x = SibSp
                             , y = Survived))
barSibSpSurv <- barSibSpSurv + stat_summary(fun.y="mean", geom="bar")
barSibSpSurv <- barSibSpSurv + ylim(0, 1)
barSibSpSurv

dataAll$sibspGroup <- dataAll$SibSp
dataAll$sibspGroup[dataAll$SibSp %in% c(1,2)] <- "1_2"
dataAll$sibspGroup[dataAll$SibSp %in% c(3,4)] <- "3_4"
dataAll$sibspGroup[dataAll$SibSp >= 5] <- "5+"
dataAll$sibspGroup <- myAsFactor(dataAll, "sibspGroup")
table(dataAll$sibspGroup)
checkDependence("SurvivedF", "sibspGroup")

```

##4. Multi-Collinearity & Feature Selection

Quite a few new features were engineered; mostly ratios such as _revenue per order_ or distinct counts such as _distinct count of payment methods used_. Here is a list of all the features including both the provided and the engineered:

```{r feature_engineering, echo=FALSE, cache=TRUE, message = F}

dataTrain    <- dataAll[!is.na(dataAll$Survived),]
dataAge      <- dataAll[!is.na(dataAll$Age),]
dataTrainAge <- dataAll[!is.na(dataAll$Survived) & !is.na(dataAll$Age),]

myFitVIF <- function(myDf, modelFeatures){
  myFit <- glm(SurvivedF ~ .
                  , data = myDf[, c("SurvivedF", modelFeatures)]
                  , family = binomial)
  myFitSummary <- summary(myFit)
  varInfFact <- as.data.frame(vif(myFit))
  varInfFact <- varInfFact[order(-varInfFact$GVIF),]
  output <- list(myFitSummary, varInfFact)
}


variables0 <- c("Pclass", "cabinLetter", "noCabinsGroup", "titleGroup"
                , "ageBucket", "Embarked", "logFareS", "parchGroup"
                , "sibspGroup")
allFit <- myFitVIF(dataTrainAge, variables0)
allFit

corrMatrix <- matrix(rep(NA, length(variables0)^2), nrow = length(variables0))
rownames(corrMatrix) <- variables0
colnames(corrMatrix) <- variables0
for (var1 in 1:length(variables0)){
  for (var2 in 1:length(variables0)){
    if (var1 != var2){
      var1Type <- class(dataAll[,variables0[var1]])
      var2Type <- class(dataAll[,variables0[var2]])
      corrMatrix[var1,var2] <- if (var1Type == "factor" & var2Type == "factor"){
        checkDependence(variables0[var1], variables0[var2])
      } else if (var1Type == "numeric" & var2Type == "numeric"){
        cor(dataAll[,c(variables0[var1], variables0[var2])])
      # might want to investigate possible issues with applying ANOVA to imbalanced data
        } else if (var1Type == "factor" & var2Type == "numeric"){
        myAov <- aov(dataAll[,c(variables0[var2])] ~ dataAll[,c(variables0[var1])]
                     , data = dataAll)
        summary(myAov)[[1]][["Pr(>F)"]][[1]]
      } else if (var1Type == "numeric" & var2Type == "factor"){
        myAov <- aov(dataAll[,c(variables0[var1])] ~ dataAll[,c(variables0[var2])]
                     , data = dataAll)
        summary(myAov)[[1]][["Pr(>F)"]][[1]]
      }
    }
  }
}
corrMatrix
heatmap(corrMatrix, Rowv = NA, Colv = NA, symm = T)


# Pclass vs cabinLetter -------------------------
table(dataAll[,c("Pclass", "cabinLetter")])
checkDependence("Pclass", "cabinLetter")

barCabinNullSurv <- ggplot(dataAll[!is.na(dataAll$Survived)
                                   & dataAll$cabinLetter == "N",]
                       , aes(x = Pclass
                             , y = Survived))
barCabinNullSurv <- barCabinNullSurv + stat_summary(fun.y="mean", geom="bar")
barCabinNullSurv <- barCabinNullSurv + ylim(0, 1)
barCabinNullSurv

dataAll$cabinClass <- as.character(dataAll$cabinLetter)
dataAll$cabinClass[dataAll$cabinLetter == "N"] <- as.character(dataAll$Pclass[dataAll$cabinLetter == "N"])
dataAll$cabinClass <- myAsFactor(dataAll, "cabinClass")
table(dataAll$cabinClass)

barCabinClassSurv <- ggplot(dataAll[!is.na(dataAll$Survived),]
                       , aes(x=reorder(cabinClass, -Survived, mean)
                             , y = Survived))
barCabinClassSurv <- barCabinClassSurv + stat_summary(fun.y="mean", geom="bar")
barCabinClassSurv <- barCabinClassSurv + ylim(0, 1)
barCabinClassSurv


variables1 <- c("cabinClass", "noCabinsGroup", "titleGroup"
                , "ageBucket", "Embarked", "logFareS", "parchGroup"
                , "sibspGroup")
dataTrain    <- dataAll[!is.na(dataAll$Survived),]
dataAge      <- dataAll[!is.na(dataAll$Age),]
dataTrainAge <- dataAll[!is.na(dataAll$Survived) & !is.na(dataAll$Age),]
allFit1 <- myFitVIF(dataTrainAge, variables1)
allFit1
# AIC actually improved
# some VIF still high


# ageBucket vs titleGroup -----------------------
table(dataAge[, c("titleGroup", "ageBucket")])
table(dataTrainAge[, c("titleGroup", "ageBucket")])

variables2 <- c("cabinClass", "noCabinsGroup", "titleGroup"
                , "Embarked", "logFareS", "parchGroup"
                , "sibspGroup")
allFit2 <- myFitVIF(dataTrainAge, variables2)
allFit2
# remove Age from Model. now switch to full data set
# because missing age is no longer issue


# full data (no age missing) --------------------
allFit2_ <- myFitVIF(dataTrain, variables2)
allFit2_
# remove Age from Model. now switch to full data set
# because missing age is no longer issue


# removing cabinClass since high VIF ------------
variables3 <- variables2[!(variables2 %in% "cabinClass")]
allFit3 <- myFitVIF(dataTrain, variables3)
allFit3
# suddenly logFareS is very significant in model.
# Try removing that and re-introduce cabinClass


# removing logFareS -----------------------------
variables4 <- variables2[!(variables2 %in% "logFareS")]
allFit4 <- myFitVIF(dataTrain, variables4)
allFit4
# remove Age from Model. now switch to full data set
# because missing age is no longer issue


# adding interaction term logFareS:cabinClass ---
myFitInteraction1 <- glm(SurvivedF ~ . + cabinClass:logFareS - cabinClass - logFareS
                         , data = dataTrain[, c("SurvivedF", variables2)]
                         , family = binomial)
myFitInteraction1Summary <- summary(myFitInteraction1)
varInfFact1 <- as.data.frame(vif(myFitInteraction1))
varInfFact1 <- varInfFact1[order(-varInfFact1$GVIF),]
output1 <- list(myFitInteraction1Summary, varInfFact1)
output1


# removing Embarked -----------------------------
variables5 <- variables2[!(variables2 %in% "Embarked")]
myFitInteraction2 <- glm(SurvivedF ~ . + cabinClass:logFareS - cabinClass - logFareS
             , data = dataTrain[, c("SurvivedF", variables5)]
             , family = binomial)
myFitInteraction2Summary <- summary(myFitInteraction2)
varInfFact2 <- as.data.frame(vif(myFitInteraction2))
varInfFact2 <- varInfFact2[order(-varInfFact2$GVIF),]
output2 <- list(myFitInteraction2Summary, varInfFact2)
output2

```


Of these, an initial selection of features was made. At this point, the excluded features were ones which are obviously correlated to another feature, or ones which were considered as not robust enough.

Next, it was made sure that all features were scaled to the range [0,1]. Most of the features were already percentages in the desired range, so those were untouched. For the ones that had larger ranges, a sigmoid function was applied.

Let $x$ be the original column and let $y$ be the normalised column. Then the normalising equation is in the form:

$y = ({1}/({1+x}))^p$

where $p$ is selected such that the median of the particular column is normalised to $0.5$.

```{r feature_scaling_and_initial_selection, echo=FALSE, cache=TRUE, message = F}


```


##5. Inferring the customer gender - Refined model: Defining Criteria

It is important to note that the inferred gender from the Refined Model differs from those of the Baseline Model by only bla bla. This is good news because the Baseline Model should already have been a good start, so a redical change by the Refined Model would have raised questions. The fact that the male/female split has not changed much either is also a positive sign. These figures could suggest that the Refined Model is indeed a "refinement".




```{r model_selection, echo=FALSE, cache=TRUE, message = F}



```

Taking into consideration the results above and the efficiency of the algorithm, it was deemed that the best performing model was the Neural Network, even though each algorithm was highly accurate. Below is the summary of the Neural Network Model performance.


```{r best_model, echo=FALSE, cache=TRUE, message = F}



```


##10. Summary


test 1 2 3


#### Recommendation for additional features


1. test1
2. test2


#### Suggested improvements for model performance

Here are just a few ideas on how the results could be improved further:

* Further investigation is needed on the data cleaning part, to have complete reconciliations
* Consulting industry experts to improve criterias for the _Core Labelled Subset_